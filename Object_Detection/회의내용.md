이제 프로젝트에서 돈나오는걸로 밥먹었으니까 그 밥먹은거 회의비로 돌려야 되는데 회의 내용을 밥먹은만큼 작성해야 하거든.
지금까지 활동한 내역들 적을테니까 일단 본 다음에 바로 적지말고 각 활동내용 잘 다듬은 다음에 회차별로 작성해야하니까 그 순서를 잘 조정해. 또, 적은 내용 말고 추가로 적을만한 내용 있으면 적어. 총 13회차 까지 적어야돼.
각 회의 내용 양식은 아래와 같이 적는데 문항은 3~4까지 정도만 적어.
```txt
1) 내용~
2) 내용~
3) 내용~
4) 내용~
```

---
## 현재까지 실습한거
- 농장 영상, 사진 데이터를 가지고 roboflow를 사용해 직접 레이블(0:닭, 1:모이통, 2: 사람) YOLO 모델 직접파인튜닝(데이터 수집, 전처리, 학습 전부 직접 하며 YOLO가 학습하는 과정 확인)
- 팀원(4명)각자 학습시킨 모델 가지고 탐지한 결과 100프로 탐지하지는 못함.
- YOLO에 대해 이론적 토의
- 닭이 죽었다고 판단할 기준 세우기(한 자리에 오래 있기, 죽은 닭의 특징을 파악)
- 이전에 대학원생이 학습시킨 모델이 성능이 좋아 그거 가지고 앞으로 실습함.(하지만 전체화면에 대해 탐지를 하면 구석은 탐지하지 못함.)
- 전체 영상을 분할해서 탐지. 어떤 비율로 분할할까 여러 가지 실험(4:3, 5:5 등)
- 탐지할 프레임 수도 조정하면서 실험(매 프레임을 탐지하면 성능은 좋은데 계산 비용이 너무 높음)
- 결국 닭이 죽었다를 판단하려면 시간이 지남에 따라 한 자리에 오래 있으면 죽었다고 판단(다른 방법이 없는지 생각)
- 현재 상황과 다르지만 비슷한 논문들을 찾으며 아이디어 탐색.
- IOU로 두 프레임간 객체의 위치를 비교해서 두 객체가 같은 객체라고 보는데 이 IOU에 대한 토론.
- IOU 임계값을 몇으로 두고 탐지해야 트래킹을 잘 하는지 실험.
- 닭의 ID를 어떤 자료구조로 관리하면 좋을지(리스트, 딕셔너리, 집합 등). 자료구조에 대해 파보기.
- 원 데이터로 탐지하려면 픽셀값이 크니까 이진 영상으로 변환해서 움직이는건 까맣게, 움직이지 않는건 하얗게 처리해 계산 비용 절감.
- 분할한 프레임을 개별 탐지 후 다시 전체 영상으로 변환하면 구석까지 잡음.
- 죽은닭 사진을 싹다 가져오고 데이터 증강을 한 다음 학습시키면 되지 않을까에 대한 토론
- 폐사체 데이터 증강 학습은 현재 상황(카메라가 위쪽에 있고 지상과 거리가 있어서 닭의 세밀한 부분까지는 파악이 힘듦.)에는 힘들다고 판단.
- 1회차 (detect_and_track3Letgo.py): 영상 4분할 후 YOLO로 닭 탐지, 중심 좌표 변화가 3픽셀 이내일 때 '정지'로 판단하여 '죽은 닭 후보' 식별

- 2회차 (사망 확률 추적): YOLO와 ByteTrack 추적기 결합, 움직임 멈춤 시간에 따라 단계별 사망 확률(30초 40%, 1분 50% 등) 계산 및 시각화

- 3회차 (폐사체 감지): 배경 차분법과 정적 픽셀 분석 도입, IoU 기반 추적 및 멀티 GPU 지원으로 고속 처리, MOG2와 IoU 기준으로 폐사체 판정

- 4회차 (0513 버전): 영상을 3x3 그리드로 분할하여 각 영역별 독립적 분석, 움직임 없는 닭 자동 식별 후 CSV 기록 및 영상으로 결과 병합

- 5회차 (0520 버전): YOLO 추적과 IoU 매칭을 결합한 다양한 ID 관리 전략 구현, 선택 영역 집중 분석으로 추적 정확도와 처리 효율 향상

- 6회차 (특정 영역 테스트): 관심 영역(ROI) 집중 분석과 배경 서브트랙션으로 정적 픽셀 비율 계산, 상태별 색상 코드(초록/파랑/빨강)로 직관적 시각화